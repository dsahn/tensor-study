# ML lec 5-1: Logistic Classification의 가설 함수 정의

## review
- hypothesis
- cost
- gradient descent

## lr vs classification

- facebook feed, spam detection, Fraud detection
- 0,1 encoding을 사용하여 분류함
- 공부시간 - 합격 그래프 예시
- linear regression과 비슷한 접근법을 사용함
  - 차이 : 합격임에도 불구하고 불합격으로 인식되는 결과가 나올수도 있다
  - wx+b 꼴을 압축시켜주는걸로 하자
  - h(x) = wx+b --> z = wx+b
    - g(z) = 0~1
    - sigmoid function(logistic function)
- logistic hypothesis : hx = 1/(1+e^(-WTX))
- cost function : 울퉁불퉁한 모양(시그모이드의 모양이 반영됨)
    - 로컬 미니멈에 빠질 수 있다
    - cost(W) = (1/m)*sum(c(h(x),y))
    - c(hx, y)= -log(hx):y=1
    - c(hx, y)= -log(1-hx):y=0
    - y=1일때 맞음, 틀림, y=0일때 맞음, 틀림을 통해서 생각해보면 됨
    - ==> C(hx,y)=ylog(hx)-(1-y)log(1-hx)
    - tf에서는 그대로쓰면됨