# ML lab 02

 건너뛰고 3을 들었네.. 다시 한번 들어보기..

# ML lec 03 - Linear Regression의 cost 최소화 알고리즘의 원리 설명

- H(x) = Wx+b
- cost(w,b) = 1/m * sum(h(x)-y)^2  ==> 최소화하기
    - cost(w,b) 는 이변수함수네?
- 최소값 찾기 문제
- 경사 하강법(gradient descent algorithm)으로 점진적으로 찾아 나감
    - 아무 점에서나 시작
    - cost(w,b)를 줄이려고 계속 값 변화 
    - alpha : learning rate
    - cost := 1/2 * cost
    - W := W - alpha * detW(cost(W))
        - 음의 기울기면 양으로, 양이면 음으로 w 이동한다는 의미네..
    - convex function
        - cost function의 3차원([w,b,cost]) 양태가 어떻게 나올것인가
        - 다른 극소값으로 갈수 있지 않나??
        - 그렇지 않다.. 그릇모양 그래프(convex function)가 나옴
   
# ML lab 03 - Linear Regression 의 cost 최소화의 TensorFlow 구현 (new)

책으로 봤던 내용이라 대략적으로만 적어봐야겠다..

