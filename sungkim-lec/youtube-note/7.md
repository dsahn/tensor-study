# 학습 rate, Overfitting, 그리고 일반화 (Regularization)

## alpha(learning rate) 구하는걸 어떻게 할거니?
- large learning rate : overshooting
    - 그래프 내에서 이동하는 거린데 이게 크면 최소점을 못찾고 방황하는 짓을 할텐데.. 밖으로 나갈지도.
    - 숫자가 아닌게 나오는것도 이때문(NaN)
- small learning rate : local minimum 찾기 너무 힘들다..

## data(x) preprocessing for gradient descent
- 등고선 형태의 w1, w2가 있는 그래프를 생각해보자
- 가운데 가장 낮은 지점을 찾는게 목표다.
- 동심원 형태의 convex가 아니라 찌그러진 형태도 있을 수 있다.
- **normalize** 하자
- zero-centered data로-> 영점 맞춰주기
- 어떠한 범위 안으로 잘 들어가도록..
- 학습이 잘 안이루어지고 겉돌때 확인해보자
- x'_j = (xj-mj)/sigma_j
  - 데이터를 평균으로 빼고 분산으로 나누자
  - x_std[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()
  
 ## overfitting
 - 우리의 모델이 트레이닝 데이터 셋에서는 잘된다!
   - **그러나 실제 데이터나 테스트 데이터 셋에서는 잘 안된다!!!**
- 많은 트레이닝 데이터!
- feature 개수 줄이기
- regularization
    - let's not have too big numbers in the weight
    - cost = (1/n)*(...) + *reg_strength* * sum(W^2)
    
    